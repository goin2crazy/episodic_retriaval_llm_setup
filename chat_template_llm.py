from transformers import TextIteratorStreamer
from threading import Thread
import torch
from concurrent.futures import ThreadPoolExecutor
import time

from memory_retrieval import *
from episodic_attention import * 
from chat import create_system_prompt

class Assistant:
    def __init__(self, 
                 processor, 
                 model, 
                 event_memory:EventMemory, 
                 k_memmories = 4, 
                 top_k_memmories = 4, 
                 ):
        """
        Initialize the Assistant with a tokenizer processor and the LLM model.

        :param processor: The processor containing tokenizer and related utilities.
        :param model: The language model to generate responses.
        """
        self.processor = processor
        self.model = model

        self.event_memory = event_memory
        self.k = k_memmories
        self.time_k = top_k_memmories

        self.conversation = []

    def update_system_prompt(self, user_input):
        """Retrieve similar events and create a system prompt."""
        retrieved_events = self.event_memory.retrieve_similar_events(user_input, k=self.k)
        retrieved_memmories_text = [entry[2] for entry in retrieved_events]
        retrieved_memmories_time = [entry[1] for entry in retrieved_events]

        # sort str times first newest, last latest and take the self.time_k from newest
        top_k_ids = sorted(range(len(retrieved_memmories_time)), key=lambda k: retrieved_memmories_time[k])[-self.time_k:]
        retrieved_memmories_text = [retrieved_memmories_text[i] for i in top_k_ids]
        retrieved_memmories_time = [retrieved_memmories_time[i] for i in top_k_ids]

        system_prompt = create_system_prompt(retrieved_memmories_text, retrieved_memmories_time)
        self.conversation = [*self.conversation[:-1], {'role': 'system', 'content': system_prompt}]


    def process_message(self, message):
        """
        Tokenize and prepare conversation inputs for the model.

        :param message: The user's input message.
        :return: Generation arguments and the streamer object.
        """
        # Add the user message to the conversation
        self.conversation.append({'role': 'user', 'content': message})
        conversation_template = self.processor.tokenizer.apply_chat_template(self.conversation, tokenize=False)
        conversation_text = conversation_template + "\n<|im_start|>assistant\n"

        print(self.conversation)

        streamer = TextIteratorStreamer(self.processor.tokenizer, skip_prompt=True, skip_special_tokens=True)
        inputs = self.processor.tokenizer(conversation_text, return_tensors='pt')

        generation_kwargs = dict(
            **inputs,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.95,
            repetition_penalty=1.15,
            streamer=streamer,
            pad_token_id=self.processor.tokenizer.eos_token_id,
            eos_token_id=self.processor.tokenizer.eos_token_id,
        )
        return generation_kwargs, streamer

    def update_conversation(self, assistant_response):
        """
        Add the assistant's response to the conversation.

        :param assistant_response: The response generated by the assistant.
        """
        self.conversation.append({'role': 'assistant', 'content': assistant_response})


    def remember_for_long(self, message:str):

        # There is possible to write code to store messages into events and remember them somewhere
        return None

    def start_conversation(self):
        """
        Begin the conversation loop, taking user input and generating responses.
        """
        while True:
            user_input = self.user_input()  # Get user input
            self.update_system_prompt(user_input)  # Update system prompt

            generation_kwargs, streamer = self.process_message(user_input)  # Process message

            # Remember somewhere
            self.remember_for_long(user_input)

            with torch.no_grad():
                thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
                thread.start()
                generated_text = ""
                for new_text in streamer:
                    generated_text += new_text


                    self.store_assistant_output(generated_text)  # Store assistant output

                    print(new_text, end='', flush=True)

                # Update the conversation with the assistant's response
                self.update_conversation(generated_text)


    # fns for in cases for further customatization
    def user_input(self,):
        return str(input("User: "))

    def store_assistant_output(self, output:str):
        return output
    

class LongMemmoryAssistant(Assistant):
    def __init__(self, episodic_config, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.episodic_config = episodic_config
        self.executor = ThreadPoolExecutor(max_workers=1)  # For background execution

    def remember_for_long(self, message: str):
        print("[Setting the boundaries refinement to background, waiting for response...]")
        def process_remember(message):
            # Minimize GPU memory usage
            with torch.no_grad():
                surprise_scores, similarity_matrix, tokens = episodic_suprise_setup_v1(
                    message, self.model, self.processor, self.episodic_config
                )
                history_events, refined_boundaries = episodic_suprise_setup_v2(
                    surprise_scores, similarity_matrix, tokens, self.episodic_config
                )

                print(f"refined {len(history_events)} events")
                for event in history_events:
                    if len(event):
                        # Store event with timestamp
                        self.event_memory.store_event(event, time.strftime("%Y-%m-%d %H:%M"))

        # Submit the process_remember task to the executor
        self.executor.submit(process_remember, message)
        print("[Background exexutiong done]")
        return None